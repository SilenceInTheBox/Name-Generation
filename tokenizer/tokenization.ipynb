{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Tokenizer - Coding along\n",
    "\n",
    "In this notebook, the first steps towards a tokenizer were done. We learnt to encode strings to bytes (using utf-8 standard) and vice-versa with `.encode()` / `.decode().` \\\n",
    "Utf-8 uses multi-byte encodings for some code points but the standard ASCII set of characters (256 characters) is a single-byte encoding (backward compatible). \\\n",
    "We end up with a list of bytes that could already be called tokens. Additionally, we perform a compression operation called _byte pair encoding_. This merges the most occuring pairs of bytes and substitutes it with a new token. Therefore, the total number of bytes decreases. As a consequence, we add a number of functions that map from the new bytes to their substitutes (for decoding). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'hello world'\n",
    "token = sent.encode(\"utf-8\")\n",
    "inttuples = list(token)\n",
    "inttuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11: hello world\n",
      "11: 104 101 108 108 111 32 119 111 114 108 100\n"
     ]
    }
   ],
   "source": [
    "print(str(len(sent)) + ': ' + sent)\n",
    "print(str(len(inttuples)) + ': ' + ' '.join(list(map(str,inttuples))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-3, 4, -3, 1, 3], {-1: (1, 1), -2: (1, 2), -3: (-1, -2)})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bpe_algorithm_old(intlist):\n",
    "    '''deprecated'''\n",
    "    newtoken = -1\n",
    "    ref = {}\n",
    "    while True:\n",
    "        inttuples = list(zip(intlist, intlist[1:]))\n",
    "        counts = {i:0 for i in set(inttuples)}\n",
    "        for i in inttuples:\n",
    "            counts[i] += 1\n",
    "\n",
    "        maxtuple = max(counts, key=lambda x: counts[x])\n",
    "        if max(counts.values()) == 1:\n",
    "            return intlist, ref\n",
    "        ref.update({newtoken: maxtuple})\n",
    "        ii = 0\n",
    "        newlist = []\n",
    "        while ii <= len(intlist)-2:\n",
    "            if (intlist[ii], intlist[ii+1]) == maxtuple:\n",
    "                newlist.append(newtoken)\n",
    "                ii +=1\n",
    "            else: \n",
    "                newlist.append(intlist[ii])\n",
    "            ii += 1\n",
    "            if ii == len(intlist)-1:\n",
    "                    newlist.append(intlist[-1])\n",
    "        intlist = newlist\n",
    "        newtoken -= 1\n",
    "\n",
    "bpe_algorithm_old([1,1,1,2,4,1,1,1,2,1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Byte pair encoding  Article Talk Read Edit View history  Tools From Wikipedia, the free encyclopedia Byte pair encoding[1][2] (also known as digram coding)[3] is an algorithm, first described in 1994 by Philip Gage for encoding strings of text into tabular form for use in downstream modeling.[4] Its modification is notable as the large language model tokenizer with an ability to combine both tokens that encode single characters (including single digits or single punctuation marks) and those that encode whole words (even the longest compound words).[5][6][7] This modification, in the first step, assumes all unique characters to be an initial set of 1-character long n-grams (i.e. initial \"tokens\"). Then, successively the most frequent pair of adjacent characters is merged into a new, 2-characters long n-gram and all instances of the pair are replaced by this new token. This is repeated until a vocabulary of prescribed size is obtained. Note that new words can always be constructed from final vocabulary tokens and initial-set characters.[8]  All the unique tokens found in a corpus are listed in a token vocabulary, the size of which, in the case of GPT-3.5 and GPT-4, is 100256.  The difference between the modified and the original algorithm is that the original algorithm does not merge the most frequent pair of bytes of data, but replaces them by a new byte that was not contained in the initial dataset. A lookup table of the replacements is required to rebuild the initial dataset. The algorithm is effective for the tokenization because it does not require large computational overheads and remains consistent and reliable.  Original algorithm The original algorithm operates by iteratively replacing the most common contiguous sequences of characters in a target piece of text with unused 'placeholder' bytes. The iteration ends when no sequences can be found, leaving the target text effectively compressed. Decompression can be performed by reversing this process, querying known placeholder terms against their corresponding denoted sequence, per a lookup table. In the original paper, this lookup table is encoded and stored alongside the compressed text.  Example Suppose the data to be encoded is  aaabdaaabac The byte pair \"aa\" occurs most often, so it will be replaced by a byte that is not used in the data, such as \"Z\". Now there is the following data and replacement table:  ZabdZabac Z=aa Then the process is repeated with byte pair \"ab\", replacing it with \"Y\":  ZYdZYac Y=ab Z=aa The only literal byte pair left occurs only once, and the encoding might stop here. Alternatively, the process could continue with recursive byte pair encoding, replacing \"ZY\" with \"X\":  XdXac X=ZY Y=ab Z=aa This data cannot be compressed further by byte pair encoding because there are no pairs of bytes that occur more than once.  To decompress the data, simply perform the replacements in the reverse order.\n",
      "2925\n",
      "[66, 121, 116, 101, 32, 112, 97, 105, 114, 32, 101, 110, 99, 111, 100, 105, 110, 103, 32, 32, 65, 114, 116, 105, 99, 108, 101, 32, 84, 97, 108, 107, 32, 82, 101, 97, 100, 32, 69, 100, 105, 116, 32, 86, 105, 101, 119, 32, 104, 105, 115, 116, 111, 114, 121, 32, 32, 84, 111, 111, 108, 115, 32, 70, 114, 111, 109, 32, 87, 105, 107, 105, 112, 101, 100, 105, 97, 44, 32, 116, 104, 101, 32, 102, 114, 101, 101, 32, 101, 110, 99, 121, 99, 108, 111, 112, 101, 100, 105, 97, 32, 66, 121, 116, 101, 32, 112, 97, 105, 114, 32, 101, 110, 99, 111, 100, 105, 110, 103, 91, 49, 93, 91, 50, 93, 32, 40, 97, 108, 115, 111, 32, 107, 110, 111, 119, 110, 32, 97, 115, 32, 100, 105, 103, 114, 97, 109, 32, 99, 111, 100, 105, 110, 103, 41, 91, 51, 93, 32, 105, 115, 32, 97, 110, 32, 97, 108, 103, 111, 114, 105, 116, 104, 109, 44, 32, 102, 105, 114, 115, 116, 32, 100, 101, 115, 99, 114, 105, 98, 101, 100, 32, 105, 110, 32, 49, 57, 57, 52, 32, 98, 121, 32, 80, 104, 105, 108, 105, 112, 32, 71, 97, 103, 101, 32, 102, 111, 114, 32, 101, 110, 99, 111, 100, 105, 110, 103, 32, 115, 116, 114, 105, 110, 103, 115, 32, 111, 102, 32, 116, 101, 120, 116, 32, 105, 110, 116, 111, 32, 116, 97, 98, 117, 108, 97, 114, 32, 102, 111, 114, 109, 32, 102, 111, 114, 32, 117, 115, 101, 32, 105, 110, 32, 100, 111, 119, 110, 115, 116, 114, 101, 97, 109, 32, 109, 111, 100, 101, 108, 105, 110, 103, 46, 91, 52, 93, 32, 73, 116, 115, 32, 109, 111, 100, 105, 102, 105, 99, 97, 116, 105, 111, 110, 32, 105, 115, 32, 110, 111, 116, 97, 98, 108, 101, 32, 97, 115, 32, 116, 104, 101, 32, 108, 97, 114, 103, 101, 32, 108, 97, 110, 103, 117, 97, 103, 101, 32, 109, 111, 100, 101, 108, 32, 116, 111, 107, 101, 110, 105, 122, 101, 114, 32, 119, 105, 116, 104, 32, 97, 110, 32, 97, 98, 105, 108, 105, 116, 121, 32, 116, 111, 32, 99, 111, 109, 98, 105, 110, 101, 32, 98, 111, 116, 104, 32, 116, 111, 107, 101, 110, 115, 32, 116, 104, 97, 116, 32, 101, 110, 99, 111, 100, 101, 32, 115, 105, 110, 103, 108, 101, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 32, 40, 105, 110, 99, 108, 117, 100, 105, 110, 103, 32, 115, 105, 110, 103, 108, 101, 32, 100, 105, 103, 105, 116, 115, 32, 111, 114, 32, 115, 105, 110, 103, 108, 101, 32, 112, 117, 110, 99, 116, 117, 97, 116, 105, 111, 110, 32, 109, 97, 114, 107, 115, 41, 32, 97, 110, 100, 32, 116, 104, 111, 115, 101, 32, 116, 104, 97, 116, 32, 101, 110, 99, 111, 100, 101, 32, 119, 104, 111, 108, 101, 32, 119, 111, 114, 100, 115, 32, 40, 101, 118, 101, 110, 32, 116, 104, 101, 32, 108, 111, 110, 103, 101, 115, 116, 32, 99, 111, 109, 112, 111, 117, 110, 100, 32, 119, 111, 114, 100, 115, 41, 46, 91, 53, 93, 91, 54, 93, 91, 55, 93, 32, 84, 104, 105, 115, 32, 109, 111, 100, 105, 102, 105, 99, 97, 116, 105, 111, 110, 44, 32, 105, 110, 32, 116, 104, 101, 32, 102, 105, 114, 115, 116, 32, 115, 116, 101, 112, 44, 32, 97, 115, 115, 117, 109, 101, 115, 32, 97, 108, 108, 32, 117, 110, 105, 113, 117, 101, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 32, 116, 111, 32, 98, 101, 32, 97, 110, 32, 105, 110, 105, 116, 105, 97, 108, 32, 115, 101, 116, 32, 111, 102, 32, 49, 45, 99, 104, 97, 114, 97, 99, 116, 101, 114, 32, 108, 111, 110, 103, 32, 110, 45, 103, 114, 97, 109, 115, 32, 40, 105, 46, 101, 46, 32, 105, 110, 105, 116, 105, 97, 108, 32, 34, 116, 111, 107, 101, 110, 115, 34, 41, 46, 32, 84, 104, 101, 110, 44, 32, 115, 117, 99, 99, 101, 115, 115, 105, 118, 101, 108, 121, 32, 116, 104, 101, 32, 109, 111, 115, 116, 32, 102, 114, 101, 113, 117, 101, 110, 116, 32, 112, 97, 105, 114, 32, 111, 102, 32, 97, 100, 106, 97, 99, 101, 110, 116, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 32, 105, 115, 32, 109, 101, 114, 103, 101, 100, 32, 105, 110, 116, 111, 32, 97, 32, 110, 101, 119, 44, 32, 50, 45, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 32, 108, 111, 110, 103, 32, 110, 45, 103, 114, 97, 109, 32, 97, 110, 100, 32, 97, 108, 108, 32, 105, 110, 115, 116, 97, 110, 99, 101, 115, 32, 111, 102, 32, 116, 104, 101, 32, 112, 97, 105, 114, 32, 97, 114, 101, 32, 114, 101, 112, 108, 97, 99, 101, 100, 32, 98, 121, 32, 116, 104, 105, 115, 32, 110, 101, 119, 32, 116, 111, 107, 101, 110, 46, 32, 84, 104, 105, 115, 32, 105, 115, 32, 114, 101, 112, 101, 97, 116, 101, 100, 32, 117, 110, 116, 105, 108, 32, 97, 32, 118, 111, 99, 97, 98, 117, 108, 97, 114, 121, 32, 111, 102, 32, 112, 114, 101, 115, 99, 114, 105, 98, 101, 100, 32, 115, 105, 122, 101, 32, 105, 115, 32, 111, 98, 116, 97, 105, 110, 101, 100, 46, 32, 78, 111, 116, 101, 32, 116, 104, 97, 116, 32, 110, 101, 119, 32, 119, 111, 114, 100, 115, 32, 99, 97, 110, 32, 97, 108, 119, 97, 121, 115, 32, 98, 101, 32, 99, 111, 110, 115, 116, 114, 117, 99, 116, 101, 100, 32, 102, 114, 111, 109, 32, 102, 105, 110, 97, 108, 32, 118, 111, 99, 97, 98, 117, 108, 97, 114, 121, 32, 116, 111, 107, 101, 110, 115, 32, 97, 110, 100, 32, 105, 110, 105, 116, 105, 97, 108, 45, 115, 101, 116, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 46, 91, 56, 93, 32, 32, 65, 108, 108, 32, 116, 104, 101, 32, 117, 110, 105, 113, 117, 101, 32, 116, 111, 107, 101, 110, 115, 32, 102, 111, 117, 110, 100, 32, 105, 110, 32, 97, 32, 99, 111, 114, 112, 117, 115, 32, 97, 114, 101, 32, 108, 105, 115, 116, 101, 100, 32, 105, 110, 32, 97, 32, 116, 111, 107, 101, 110, 32, 118, 111, 99, 97, 98, 117, 108, 97, 114, 121, 44, 32, 116, 104, 101, 32, 115, 105, 122, 101, 32, 111, 102, 32, 119, 104, 105, 99, 104, 44, 32, 105, 110, 32, 116, 104, 101, 32, 99, 97, 115, 101, 32, 111, 102, 32, 71, 80, 84, 45, 51, 46, 53, 32, 97, 110, 100, 32, 71, 80, 84, 45, 52, 44, 32, 105, 115, 32, 49, 48, 48, 50, 53, 54, 46, 32, 32, 84, 104, 101, 32, 100, 105, 102, 102, 101, 114, 101, 110, 99, 101, 32, 98, 101, 116, 119, 101, 101, 110, 32, 116, 104, 101, 32, 109, 111, 100, 105, 102, 105, 101, 100, 32, 97, 110, 100, 32, 116, 104, 101, 32, 111, 114, 105, 103, 105, 110, 97, 108, 32, 97, 108, 103, 111, 114, 105, 116, 104, 109, 32, 105, 115, 32, 116, 104, 97, 116, 32, 116, 104, 101, 32, 111, 114, 105, 103, 105, 110, 97, 108, 32, 97, 108, 103, 111, 114, 105, 116, 104, 109, 32, 100, 111, 101, 115, 32, 110, 111, 116, 32, 109, 101, 114, 103, 101, 32, 116, 104, 101, 32, 109, 111, 115, 116, 32, 102, 114, 101, 113, 117, 101, 110, 116, 32, 112, 97, 105, 114, 32, 111, 102, 32, 98, 121, 116, 101, 115, 32, 111, 102, 32, 100, 97, 116, 97, 44, 32, 98, 117, 116, 32, 114, 101, 112, 108, 97, 99, 101, 115, 32, 116, 104, 101, 109, 32, 98, 121, 32, 97, 32, 110, 101, 119, 32, 98, 121, 116, 101, 32, 116, 104, 97, 116, 32, 119, 97, 115, 32, 110, 111, 116, 32, 99, 111, 110, 116, 97, 105, 110, 101, 100, 32, 105, 110, 32, 116, 104, 101, 32, 105, 110, 105, 116, 105, 97, 108, 32, 100, 97, 116, 97, 115, 101, 116, 46, 32, 65, 32, 108, 111, 111, 107, 117, 112, 32, 116, 97, 98, 108, 101, 32, 111, 102, 32, 116, 104, 101, 32, 114, 101, 112, 108, 97, 99, 101, 109, 101, 110, 116, 115, 32, 105, 115, 32, 114, 101, 113, 117, 105, 114, 101, 100, 32, 116, 111, 32, 114, 101, 98, 117, 105, 108, 100, 32, 116, 104, 101, 32, 105, 110, 105, 116, 105, 97, 108, 32, 100, 97, 116, 97, 115, 101, 116, 46, 32, 84, 104, 101, 32, 97, 108, 103, 111, 114, 105, 116, 104, 109, 32, 105, 115, 32, 101, 102, 102, 101, 99, 116, 105, 118, 101, 32, 102, 111, 114, 32, 116, 104, 101, 32, 116, 111, 107, 101, 110, 105, 122, 97, 116, 105, 111, 110, 32, 98, 101, 99, 97, 117, 115, 101, 32, 105, 116, 32, 100, 111, 101, 115, 32, 110, 111, 116, 32, 114, 101, 113, 117, 105, 114, 101, 32, 108, 97, 114, 103, 101, 32, 99, 111, 109, 112, 117, 116, 97, 116, 105, 111, 110, 97, 108, 32, 111, 118, 101, 114, 104, 101, 97, 100, 115, 32, 97, 110, 100, 32, 114, 101, 109, 97, 105, 110, 115, 32, 99, 111, 110, 115, 105, 115, 116, 101, 110, 116, 32, 97, 110, 100, 32, 114, 101, 108, 105, 97, 98, 108, 101, 46, 32, 32, 79, 114, 105, 103, 105, 110, 97, 108, 32, 97, 108, 103, 111, 114, 105, 116, 104, 109, 32, 84, 104, 101, 32, 111, 114, 105, 103, 105, 110, 97, 108, 32, 97, 108, 103, 111, 114, 105, 116, 104, 109, 32, 111, 112, 101, 114, 97, 116, 101, 115, 32, 98, 121, 32, 105, 116, 101, 114, 97, 116, 105, 118, 101, 108, 121, 32, 114, 101, 112, 108, 97, 99, 105, 110, 103, 32, 116, 104, 101, 32, 109, 111, 115, 116, 32, 99, 111, 109, 109, 111, 110, 32, 99, 111, 110, 116, 105, 103, 117, 111, 117, 115, 32, 115, 101, 113, 117, 101, 110, 99, 101, 115, 32, 111, 102, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 32, 105, 110, 32, 97, 32, 116, 97, 114, 103, 101, 116, 32, 112, 105, 101, 99, 101, 32, 111, 102, 32, 116, 101, 120, 116, 32, 119, 105, 116, 104, 32, 117, 110, 117, 115, 101, 100, 32, 39, 112, 108, 97, 99, 101, 104, 111, 108, 100, 101, 114, 39, 32, 98, 121, 116, 101, 115, 46, 32, 84, 104, 101, 32, 105, 116, 101, 114, 97, 116, 105, 111, 110, 32, 101, 110, 100, 115, 32, 119, 104, 101, 110, 32, 110, 111, 32, 115, 101, 113, 117, 101, 110, 99, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 102, 111, 117, 110, 100, 44, 32, 108, 101, 97, 118, 105, 110, 103, 32, 116, 104, 101, 32, 116, 97, 114, 103, 101, 116, 32, 116, 101, 120, 116, 32, 101, 102, 102, 101, 99, 116, 105, 118, 101, 108, 121, 32, 99, 111, 109, 112, 114, 101, 115, 115, 101, 100, 46, 32, 68, 101, 99, 111, 109, 112, 114, 101, 115, 115, 105, 111, 110, 32, 99, 97, 110, 32, 98, 101, 32, 112, 101, 114, 102, 111, 114, 109, 101, 100, 32, 98, 121, 32, 114, 101, 118, 101, 114, 115, 105, 110, 103, 32, 116, 104, 105, 115, 32, 112, 114, 111, 99, 101, 115, 115, 44, 32, 113, 117, 101, 114, 121, 105, 110, 103, 32, 107, 110, 111, 119, 110, 32, 112, 108, 97, 99, 101, 104, 111, 108, 100, 101, 114, 32, 116, 101, 114, 109, 115, 32, 97, 103, 97, 105, 110, 115, 116, 32, 116, 104, 101, 105, 114, 32, 99, 111, 114, 114, 101, 115, 112, 111, 110, 100, 105, 110, 103, 32, 100, 101, 110, 111, 116, 101, 100, 32, 115, 101, 113, 117, 101, 110, 99, 101, 44, 32, 112, 101, 114, 32, 97, 32, 108, 111, 111, 107, 117, 112, 32, 116, 97, 98, 108, 101, 46, 32, 73, 110, 32, 116, 104, 101, 32, 111, 114, 105, 103, 105, 110, 97, 108, 32, 112, 97, 112, 101, 114, 44, 32, 116, 104, 105, 115, 32, 108, 111, 111, 107, 117, 112, 32, 116, 97, 98, 108, 101, 32, 105, 115, 32, 101, 110, 99, 111, 100, 101, 100, 32, 97, 110, 100, 32, 115, 116, 111, 114, 101, 100, 32, 97, 108, 111, 110, 103, 115, 105, 100, 101, 32, 116, 104, 101, 32, 99, 111, 109, 112, 114, 101, 115, 115, 101, 100, 32, 116, 101, 120, 116, 46, 32, 32, 69, 120, 97, 109, 112, 108, 101, 32, 83, 117, 112, 112, 111, 115, 101, 32, 116, 104, 101, 32, 100, 97, 116, 97, 32, 116, 111, 32, 98, 101, 32, 101, 110, 99, 111, 100, 101, 100, 32, 105, 115, 32, 32, 97, 97, 97, 98, 100, 97, 97, 97, 98, 97, 99, 32, 84, 104, 101, 32, 98, 121, 116, 101, 32, 112, 97, 105, 114, 32, 34, 97, 97, 34, 32, 111, 99, 99, 117, 114, 115, 32, 109, 111, 115, 116, 32, 111, 102, 116, 101, 110, 44, 32, 115, 111, 32, 105, 116, 32, 119, 105, 108, 108, 32, 98, 101, 32, 114, 101, 112, 108, 97, 99, 101, 100, 32, 98, 121, 32, 97, 32, 98, 121, 116, 101, 32, 116, 104, 97, 116, 32, 105, 115, 32, 110, 111, 116, 32, 117, 115, 101, 100, 32, 105, 110, 32, 116, 104, 101, 32, 100, 97, 116, 97, 44, 32, 115, 117, 99, 104, 32, 97, 115, 32, 34, 90, 34, 46, 32, 78, 111, 119, 32, 116, 104, 101, 114, 101, 32, 105, 115, 32, 116, 104, 101, 32, 102, 111, 108, 108, 111, 119, 105, 110, 103, 32, 100, 97, 116, 97, 32, 97, 110, 100, 32, 114, 101, 112, 108, 97, 99, 101, 109, 101, 110, 116, 32, 116, 97, 98, 108, 101, 58, 32, 32, 90, 97, 98, 100, 90, 97, 98, 97, 99, 32, 90, 61, 97, 97, 32, 84, 104, 101, 110, 32, 116, 104, 101, 32, 112, 114, 111, 99, 101, 115, 115, 32, 105, 115, 32, 114, 101, 112, 101, 97, 116, 101, 100, 32, 119, 105, 116, 104, 32, 98, 121, 116, 101, 32, 112, 97, 105, 114, 32, 34, 97, 98, 34, 44, 32, 114, 101, 112, 108, 97, 99, 105, 110, 103, 32, 105, 116, 32, 119, 105, 116, 104, 32, 34, 89, 34, 58, 32, 32, 90, 89, 100, 90, 89, 97, 99, 32, 89, 61, 97, 98, 32, 90, 61, 97, 97, 32, 84, 104, 101, 32, 111, 110, 108, 121, 32, 108, 105, 116, 101, 114, 97, 108, 32, 98, 121, 116, 101, 32, 112, 97, 105, 114, 32, 108, 101, 102, 116, 32, 111, 99, 99, 117, 114, 115, 32, 111, 110, 108, 121, 32, 111, 110, 99, 101, 44, 32, 97, 110, 100, 32, 116, 104, 101, 32, 101, 110, 99, 111, 100, 105, 110, 103, 32, 109, 105, 103, 104, 116, 32, 115, 116, 111, 112, 32, 104, 101, 114, 101, 46, 32, 65, 108, 116, 101, 114, 110, 97, 116, 105, 118, 101, 108, 121, 44, 32, 116, 104, 101, 32, 112, 114, 111, 99, 101, 115, 115, 32, 99, 111, 117, 108, 100, 32, 99, 111, 110, 116, 105, 110, 117, 101, 32, 119, 105, 116, 104, 32, 114, 101, 99, 117, 114, 115, 105, 118, 101, 32, 98, 121, 116, 101, 32, 112, 97, 105, 114, 32, 101, 110, 99, 111, 100, 105, 110, 103, 44, 32, 114, 101, 112, 108, 97, 99, 105, 110, 103, 32, 34, 90, 89, 34, 32, 119, 105, 116, 104, 32, 34, 88, 34, 58, 32, 32, 88, 100, 88, 97, 99, 32, 88, 61, 90, 89, 32, 89, 61, 97, 98, 32, 90, 61, 97, 97, 32, 84, 104, 105, 115, 32, 100, 97, 116, 97, 32, 99, 97, 110, 110, 111, 116, 32, 98, 101, 32, 99, 111, 109, 112, 114, 101, 115, 115, 101, 100, 32, 102, 117, 114, 116, 104, 101, 114, 32, 98, 121, 32, 98, 121, 116, 101, 32, 112, 97, 105, 114, 32, 101, 110, 99, 111, 100, 105, 110, 103, 32, 98, 101, 99, 97, 117, 115, 101, 32, 116, 104, 101, 114, 101, 32, 97, 114, 101, 32, 110, 111, 32, 112, 97, 105, 114, 115, 32, 111, 102, 32, 98, 121, 116, 101, 115, 32, 116, 104, 97, 116, 32, 111, 99, 99, 117, 114, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 111, 110, 99, 101, 46, 32, 32, 84, 111, 32, 100, 101, 99, 111, 109, 112, 114, 101, 115, 115, 32, 116, 104, 101, 32, 100, 97, 116, 97, 44, 32, 115, 105, 109, 112, 108, 121, 32, 112, 101, 114, 102, 111, 114, 109, 32, 116, 104, 101, 32, 114, 101, 112, 108, 97, 99, 101, 109, 101, 110, 116, 115, 32, 105, 110, 32, 116, 104, 101, 32, 114, 101, 118, 101, 114, 115, 101, 32, 111, 114, 100, 101, 114, 46]\n",
      "2925\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Byte pair encoding  Article Talk Read Edit View history  Tools From Wikipedia, the free encyclopedia Byte pair encoding[1][2] (also known as digram coding)[3] is an algorithm, first described in 1994 by Philip Gage for encoding strings of text into tabular form for use in downstream modeling.[4] Its modification is notable as the large language model tokenizer with an ability to combine both tokens that encode single characters (including single digits or single punctuation marks) and those that encode whole words (even the longest compound words).[5][6][7] This modification, in the first step, assumes all unique characters to be an initial set of 1-character long n-grams (i.e. initial \"tokens\"). Then, successively the most frequent pair of adjacent characters is merged into a new, 2-characters long n-gram and all instances of the pair are replaced by this new token. This is repeated until a vocabulary of prescribed size is obtained. Note that new words can always be constructed from final vocabulary tokens and initial-set characters.[8]  All the unique tokens found in a corpus are listed in a token vocabulary, the size of which, in the case of GPT-3.5 and GPT-4, is 100256.  The difference between the modified and the original algorithm is that the original algorithm does not merge the most frequent pair of bytes of data, but replaces them by a new byte that was not contained in the initial dataset. A lookup table of the replacements is required to rebuild the initial dataset. The algorithm is effective for the tokenization because it does not require large computational overheads and remains consistent and reliable.  Original algorithm The original algorithm operates by iteratively replacing the most common contiguous sequences of characters in a target piece of text with unused \\'placeholder\\' bytes. The iteration ends when no sequences can be found, leaving the target text effectively compressed. Decompression can be performed by reversing this process, querying known placeholder terms against their corresponding denoted sequence, per a lookup table. In the original paper, this lookup table is encoded and stored alongside the compressed text.  Example Suppose the data to be encoded is  aaabdaaabac The byte pair \"aa\" occurs most often, so it will be replaced by a byte that is not used in the data, such as \"Z\". Now there is the following data and replacement table:  ZabdZabac Z=aa Then the process is repeated with byte pair \"ab\", replacing it with \"Y\":  ZYdZYac Y=ab Z=aa The only literal byte pair left occurs only once, and the encoding might stop here. Alternatively, the process could continue with recursive byte pair encoding, replacing \"ZY\" with \"X\":  XdXac X=ZY Y=ab Z=aa This data cannot be compressed further by byte pair encoding because there are no pairs of bytes that occur more than once.  To decompress the data, simply perform the replacements in the reverse order.\"\"\"\n",
    "tokens = list(text.encode('utf-8'))\n",
    "\n",
    "print(text, len(text), sep='\\n')\n",
    "print(tokens, len(tokens), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into a new token 256\n",
      "merging (115, 32) into a new token 257\n",
      "merging (116, 104) into a new token 258\n",
      "merging (105, 110) into a new token 259\n",
      "merging (116, 32) into a new token 260\n",
      "merging (101, 110) into a new token 261\n",
      "merging (100, 32) into a new token 262\n",
      "merging (114, 101) into a new token 263\n",
      "merging (101, 114) into a new token 264\n",
      "merging (258, 256) into a new token 265\n",
      "merging (99, 111) into a new token 266\n",
      "merging (32, 97) into a new token 267\n",
      "merging (97, 99) into a new token 268\n",
      "merging (111, 114) into a new token 269\n",
      "merging (259, 103) into a new token 270\n",
      "merging (105, 257) into a new token 271\n",
      "merging (32, 265) into a new token 272\n",
      "merging (116, 97) into a new token 273\n",
      "merging (97, 108) into a new token 274\n",
      "merging (101, 262) into a new token 275\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    stats = {i:0 for i in set(zip(ids, ids[1:]))}\n",
    "    for i in zip(ids, ids[1:]):\n",
    "        stats[i] += 1\n",
    "    sorted_stats = dict(sorted(stats.items(), key=lambda x: x[1], reverse=True))\n",
    "    return sorted_stats\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    merged_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids)-1:\n",
    "        if (ids[i], ids[i+1]) == pair: \n",
    "            merged_ids += [idx]\n",
    "            i += 2\n",
    "        else:\n",
    "            merged_ids += [ids[i]]\n",
    "            i += 1\n",
    "    if i == len(ids) - 1:\n",
    "        merged_ids += [ids[i]]\n",
    "    return merged_ids\n",
    "\n",
    "#---\n",
    "vocab_size = 276\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens)\n",
    "\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    print(f\"merging {pair} into a new token {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens lengths: 2925\n",
      "ids length: 2227\n",
      "compression ratio: 1.31X\n"
     ]
    }
   ],
   "source": [
    "print('tokens lengths:', len(tokens))\n",
    "print('ids length:', len(ids))\n",
    "print(f'compression ratio: {len(tokens) / len(ids):.2f}X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104, 101, 108, 108, 111, 32, 119, 269, 108, 100]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(text):\n",
    "    tokens = list(text.encode('utf-8'))\n",
    "    while len(tokens) >= 2:\n",
    "        stats = get_stats(tokens)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float('inf')))\n",
    "        if pair not in merges:\n",
    "            break # nothing else can be merged\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "\n",
    "encode('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world!'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0,p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "    tokens = b''.join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode('utf-8', errors='replace')\n",
    "    return text\n",
    "\n",
    "\n",
    "decode(encode(\"\"\"hello world!\"\"\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
