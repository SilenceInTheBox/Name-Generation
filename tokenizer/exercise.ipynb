{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic import BasicTokenizer\n",
    "from regextok import RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185561\n"
     ]
    }
   ],
   "source": [
    "with open('../text_input/taylorswift.txt', 'r') as f:\n",
    "    ttext = f.read()\n",
    "    print(len(ttext))\n",
    "\n",
    "basictok = BasicTokenizer()\n",
    "regextok = RegexTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e|  \t (101, 32) --> 256\n",
      ",|  \t (44, 32) --> 257\n",
      "d|  \t (100, 32) --> 258\n",
      ".|  \t (46, 32) --> 259\n",
      "r|  \t (114, 32) --> 260\n",
      "2|0 \t (50, 48) --> 261\n",
      "s|  \t (115, 32) --> 262\n",
      "i|n \t (105, 110) --> 263\n",
      "o|n \t (111, 110) --> 264\n",
      "r|i \t (114, 105) --> 265\n",
      "t|  \t (116, 32) --> 266\n",
      "t|h \t (116, 104) --> 267\n",
      "e|d  \t (101, 258) --> 268\n",
      ", |20 \t (257, 261) --> 269\n",
      "a|n \t (97, 110) --> 270\n",
      "a|r \t (97, 114) --> 271\n",
      "e|r  \t (101, 260) --> 272\n",
      "y|  \t (121, 32) --> 273\n",
      "a|l \t (97, 108) --> 274\n",
      "th|e  \t (267, 256) --> 275\n",
      "v|ed  \t (118, 268) --> 276\n",
      "w|i \t (119, 105) --> 277\n",
      "e|r \t (101, 114) --> 278\n",
      "on|  \t (264, 32) --> 279\n",
      "wi|f \t (277, 102) --> 280\n",
      "R|e \t (82, 101) --> 281\n",
      "S|wif \t (83, 280) --> 282\n",
      "o|r  \t (111, 260) --> 283\n",
      "c|h \t (99, 104) --> 284\n",
      ", 20|1 \t (269, 49) --> 285\n",
      "o|m \t (111, 109) --> 286\n",
      "b|er  \t (98, 272) --> 287\n",
      " |the  \t (32, 275) --> 288\n",
      "a|y \t (97, 121) --> 289\n",
      "e|n \t (101, 110) --> 290\n",
      "o|r \t (111, 114) --> 291\n",
      "al|  \t (274, 32) --> 292\n",
      "e|m \t (101, 109) --> 293\n",
      ".|\n",
      " \t (46, 10) --> 294\n",
      "ri|e \t (265, 101) --> 295\n",
      "in|g \t (263, 103) --> 296\n",
      ", 20|2 \t (269, 50) --> 297\n",
      "t|i \t (116, 105) --> 298\n",
      "ay|l \t (289, 108) --> 299\n",
      "\"|.  \t (34, 259) --> 300\n",
      "l|l \t (108, 108) --> 301\n",
      "T|ayl \t (84, 299) --> 302\n",
      "t|rie \t (116, 295) --> 303\n",
      ".\n",
      "|  \t (294, 32) --> 304\n",
      "t|o \t (116, 111) --> 305\n",
      ". |Re \t (259, 281) --> 306\n",
      ". Re|trie \t (306, 303) --> 307\n",
      ". Retrie|ved  \t (307, 276) --> 308\n",
      "Tayl|or  \t (302, 283) --> 309\n",
      "e|s \t (101, 115) --> 310\n",
      "Taylor |Swif \t (309, 282) --> 311\n",
      "u|s \t (117, 115) --> 312\n",
      "r|om \t (114, 286) --> 313\n",
      "em|ber  \t (293, 287) --> 314\n",
      ")|.  \t (41, 259) --> 315\n",
      "A|r \t (65, 114) --> 316\n",
      "f|rom \t (102, 313) --> 317\n",
      "). |\" \t (315, 34) --> 318\n",
      "an|d  \t (270, 258) --> 319\n",
      "r|e \t (114, 101) --> 320\n",
      "o|u \t (111, 117) --> 321\n",
      "o|ri \t (111, 265) --> 322\n",
      "o|f \t (111, 102) --> 323\n",
      "g|in \t (103, 263) --> 324\n",
      "ing|  \t (296, 32) --> 325\n",
      "ch|i \t (284, 105) --> 326\n",
      "]|  \t (93, 32) --> 327\n",
      "gin|al  \t (324, 292) --> 328\n",
      "from| the  \t (317, 288) --> 329\n",
      "ori|ginal  \t (322, 328) --> 330\n",
      "h|e  \t (104, 256) --> 331\n",
      "Ar|chi \t (316, 326) --> 332\n",
      "Archi|ved  \t (332, 276) --> 333\n",
      "from the |original  \t (329, 330) --> 334\n",
      "Archived |from the original  \t (333, 334) --> 335\n",
      "Archived from the original |on  \t (335, 279) --> 336\n",
      ". |Archived from the original on  \t (259, 336) --> 337\n",
      "a|  \t (97, 32) --> 338\n",
      "s|t \t (115, 116) --> 339\n",
      "i|c \t (105, 99) --> 340\n",
      ".|[ \t (46, 91) --> 341\n",
      "e|c \t (101, 99) --> 342\n",
      "i|ll \t (105, 301) --> 343\n",
      "'|s  \t (39, 262) --> 344\n",
      "Taylor Swif|t  \t (311, 266) --> 345\n",
      "o|v \t (111, 118) --> 346\n",
      "a|t \t (97, 116) --> 347\n",
      "a|s  \t (97, 262) --> 348\n",
      "e|s  \t (101, 262) --> 349\n"
     ]
    }
   ],
   "source": [
    "basictok.train(ttext, 350, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e|r \t (101, 114) --> 256\n",
      "2|0 \t (50, 48) --> 257\n",
      "o|r \t (111, 114) --> 258\n",
      "i|n \t (105, 110) --> 259\n",
      "e|d \t (101, 100) --> 260\n",
      " |t \t (32, 116) --> 261\n",
      "o|n \t (111, 110) --> 262\n",
      "h|e \t (104, 101) --> 263\n",
      " |S \t (32, 83) --> 264\n",
      "a|r \t (97, 114) --> 265\n",
      "a|n \t (97, 110) --> 266\n",
      " |A \t (32, 65) --> 267\n",
      " t|he \t (261, 263) --> 268\n",
      "a|l \t (97, 108) --> 269\n",
      "r|i \t (114, 105) --> 270\n",
      "v|ed \t (118, 260) --> 271\n",
      "s|t \t (115, 116) --> 272\n",
      "w|i \t (119, 105) --> 273\n",
      " |R \t (32, 82) --> 274\n",
      "20|1 \t (257, 49) --> 275\n",
      " |f \t (32, 102) --> 276\n",
      "20|2 \t (257, 50) --> 277\n",
      " |T \t (32, 84) --> 278\n",
      "f|t \t (102, 116) --> 279\n",
      "a|y \t (97, 121) --> 280\n",
      " |\" \t (32, 34) --> 281\n",
      "wi|ft \t (273, 279) --> 282\n",
      "e|t \t (101, 116) --> 283\n",
      " S|wift \t (264, 282) --> 284\n",
      "c|h \t (99, 104) --> 285\n",
      "b|er \t (98, 256) --> 286\n",
      "a|t \t (97, 116) --> 287\n",
      "o|m \t (111, 109) --> 288\n",
      "e|s \t (101, 115) --> 289\n",
      "e|n \t (101, 110) --> 290\n",
      "e|m \t (101, 109) --> 291\n",
      "\"|. \t (34, 46) --> 292\n",
      " |( \t (32, 40) --> 293\n",
      ".|\n",
      " \t (46, 10) --> 294\n",
      "in|g \t (259, 103) --> 295\n",
      "l|or \t (108, 258) --> 296\n",
      " |M \t (32, 77) --> 297\n",
      "i|g \t (105, 103) --> 298\n",
      " |on \t (32, 262) --> 299\n",
      "ay|lor \t (280, 296) --> 300\n",
      "l|l \t (108, 108) --> 301\n",
      "ri|e \t (270, 101) --> 302\n",
      " R|et \t (274, 283) --> 303\n",
      " Ret|rie \t (303, 302) --> 304\n",
      " Retrie|ved \t (304, 271) --> 305\n",
      " |s \t (32, 115) --> 306\n",
      "i|c \t (105, 99) --> 307\n",
      "an|d \t (266, 100) --> 308\n",
      "o|u \t (111, 117) --> 309\n",
      "e|c \t (101, 99) --> 310\n",
      " |a \t (32, 97) --> 311\n",
      ")|. \t (41, 46) --> 312\n",
      "r|om \t (114, 288) --> 313\n",
      " |B \t (32, 66) --> 314\n",
      "em|ber \t (291, 286) --> 315\n",
      " |o \t (32, 111) --> 316\n",
      " f|rom \t (276, 313) --> 317\n",
      " A|r \t (267, 114) --> 318\n",
      " |and \t (32, 308) --> 319\n",
      " |C \t (32, 67) --> 320\n",
      " |N \t (32, 78) --> 321\n",
      " |or \t (32, 258) --> 322\n",
      "ch|i \t (285, 105) --> 323\n",
      " |J \t (32, 74) --> 324\n",
      "in|al \t (259, 269) --> 325\n",
      " or|ig \t (322, 298) --> 326\n",
      " orig|inal \t (326, 325) --> 327\n",
      " Ar|chi \t (318, 323) --> 328\n",
      " Archi|ved \t (328, 271) --> 329\n",
      " o|f \t (316, 102) --> 330\n",
      " |h \t (32, 104) --> 331\n",
      " |in \t (32, 259) --> 332\n",
      "r|e \t (114, 101) --> 333\n",
      "T|aylor \t (84, 300) --> 334\n",
      "i|t \t (105, 116) --> 335\n",
      "a|s \t (97, 115) --> 336\n",
      " |p \t (32, 112) --> 337\n",
      "i|on \t (105, 262) --> 338\n",
      " |D \t (32, 68) --> 339\n",
      " |w \t (32, 119) --> 340\n",
      "ar|d \t (265, 100) --> 341\n",
      "i|ll \t (105, 301) --> 342\n",
      "'|s \t (39, 115) --> 343\n",
      " |m \t (32, 109) --> 344\n",
      " |F \t (32, 70) --> 345\n",
      " |W \t (32, 87) --> 346\n",
      "l|e \t (108, 101) --> 347\n",
      " t|o \t (261, 111) --> 348\n",
      " |c \t (32, 99) --> 349\n"
     ]
    }
   ],
   "source": [
    "regextok.train(ttext, 350, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([263,\n",
       "  301,\n",
       "  111,\n",
       "  340,\n",
       "  258,\n",
       "  108,\n",
       "  100,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  63,\n",
       "  293,\n",
       "  236,\n",
       "  149,\n",
       "  136,\n",
       "  235,\n",
       "  133,\n",
       "  149,\n",
       "  237,\n",
       "  149,\n",
       "  152,\n",
       "  236,\n",
       "  132,\n",
       "  184,\n",
       "  236,\n",
       "  154,\n",
       "  148,\n",
       "  33,\n",
       "  41,\n",
       "  32,\n",
       "  108,\n",
       "  111,\n",
       "  108,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  32,\n",
       "  240,\n",
       "  159,\n",
       "  152,\n",
       "  137],\n",
       " True,\n",
       " 33,\n",
       " 41)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = 'hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ðŸ˜‰'\n",
    "bb = regextok.encode(aa)\n",
    "cc = regextok.decode(bb)\n",
    "bb, aa == cc, len(aa), len(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 1115394\n",
      "length of utf-8 encoding: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('../text_input/input.txt', 'r') as f:\n",
    "    lon_val  = f.read()\n",
    "\n",
    "\n",
    "sho_val = \"\"\"## GPT Tokenizer - Coding along\n",
    "In this notebook, the first steps towards a tokenizer were done. We learnt to encode strings to bytes (using utf-8 standard) and vice-versa with `.encode()` / `.decode().` \\\n",
    "Utf-8 uses multi-byte encodings for some code points but the standard ASCII set of characters (256 characters) is a single-byte encoding (backward compatible). \\\n",
    "We end up with a list of bytes that could already be called tokens. Additionally, we perform a compression operation called _byte pair encoding_. This merges the most occuring pairs of bytes and substitutes it with a new token. Therefore, the total number of bytes decreases. As a consequence, we add a number of functions that map from the new bytes to their substitutes (for decoding). \"\"\"\n",
    "\n",
    "valtext = lon_val\n",
    "\n",
    "valids = list(valtext.encode('utf-8'))\n",
    "\n",
    "print('length of text:', len(valtext))\n",
    "print('length of utf-8 encoding:', len(valids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tokenizer\n",
      "length of tokens: 949290\n",
      "compression rate: 1.1750\n",
      "\n",
      "Regex tokenizer\n",
      "length of tokens: 824813\n",
      "compression rate: 1.3523\n"
     ]
    }
   ],
   "source": [
    "basicval = basictok.encode(valtext)\n",
    "regexval = regextok.encode(valtext)\n",
    "\n",
    "print('Basic tokenizer')\n",
    "print('length of tokens:', len(basicval))\n",
    "print(f'compression rate: {len(valtext)/len(basicval) :.4f}\\n')\n",
    "\n",
    "print('Regex tokenizer')\n",
    "print('length of tokens:', len(regexval))\n",
    "print(f'compression rate: {len(valtext)/len(regexval) :.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
